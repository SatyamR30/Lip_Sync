
# Lip-Syncing Model using Wav2Lip

## Overview

This project implements a lip-syncing model using the Wav2Lip algorithm. It allows you to synchronize the lip movements of a character in a video with an input audio source. This can be particularly useful for dubbing, video editing, or creating realistic deepfake videos.



## Usage

Explain how to use the lip-syncing model. Provide step-by-step instructions:-
1. First load the model.
2. Then download the given inputs provided in this repo.
3. You can add the padding and change the scaling factor 
4. After running the model, you will get final results



##Inputs

These are the inputs of the audio and video that I chose for this  model
- `--video`: [Input_video](https://drive.google.com/file/d/1dRZdSm9hzx_SXfHmkHdkGx3FXx5LBirR/view)
- `--audio`: [Input_audio](https://drive.google.com/file/d/1CrwhSkCSyLNCQTixXju7rRA-41Bf0fyb/view?usp=sharing)

##Ouput
- `--output`: [Output_video](https://drive.google.com/file/d/1Ov3OUfx5VMeEYu25uvGOSWPOwth-EEAT/view?usp=sharing)
